
# 1 introduction-tensorflow

## 1.1 a-new-programming-paradigm

## 1.1.1_a-new-programming-paradigm
- [x] 1.1.1.1 introduction a conversation with andrew ng.en.srt
- [x] 1.1.1.3 a primer in machine learning.en.srt
- [ ] 1.1.1.4 the hello world of neural networks.en.srt
- [ ] 1.1.1.6 working through hello world in tensorflow and python.en.srt

## 1.1.2_weekly-exercise-your-first-neural-network

## 1.2 introduction-to-computer-vision

## 1.2.1_introduction-to-computer-vision
- [x] 1.2.1.1 a conversation with andrew ng.en.srt
- [x] 1.2.1.10 using callbacks to control training.en.srt
- [x] 1.2.1.12 walk through a notebook with callbacks.en.srt
- [x] 1.2.1.2 an introduction to computer vision.en.srt
- [x] 1.2.1.4 writing code to load training data.en.srt
- [x] 1.2.1.6 coding a computer vision neural network.en.srt
- [x] 1.2.1.8 walk through a notebook for computer vision.en.srt

## 1.2.2_weekly-exercise-implement-a-deep-neural-network-to-recognize-handwritten-digits

## 1.3 enhancing-vision-with-convolutional-neural-networks

## 1.3.1_enhancing-vision-with-convolutional-neural-networks
- [x] 1.3.1.1 a conversation with andrew ng.en.srt
- [x] 1.3.1.10 walking through convolutions.en.srt
- [x] 1.3.1.2 what are convolutions and pooling.en.srt
- [x] 1.3.1.4 implementing convolutional layers.en.srt
- [ ] 1.3.1.6 implementing pooling layers.en.srt
- [ ] 1.3.1.8 improving the fashion classifier with convolutions.en.srt

## 1.3.2_weekly-exercise-improving-dnn-performance-using-convolutions

## 1.4 using-real-world-images

## 1.4.1_using-real-world-images
- [ ] 1.4.1.1 a conversation with andrew ng.en.srt
- [x] 1.4.1.11 walking through training the convnet with fit generator.en.srt
- [ ] 1.4.1.13 adding automatic validation to test accuracy.en.srt
- [x] 1.4.1.15 exploring the impact of compressing images.en.srt
- [x] 1.4.1.3 understanding imagegenerator.en.srt
- [ ] 1.4.1.5 defining a convnet to use complex images.en.srt
- [x] 1.4.1.7 training the convnet with fit generator.en.srt
- [x] 1.4.1.9 walking through developing a convnet.en.srt

## 1.4.2_weekly-exercise-handling-complex-images

## 1.4.3_course-1-wrap-up
- [x] 1.4.3.2 a conversation with andrew.en.srt

# 2 convolutional-neural-networks-tensorflow

## 2.1 exploring-a-larger-dataset

## 2.1.1_introduction
- [x] 2.1.1.1 introduction a conversation with andrew ng.en.srt

## 2.1.2_larger-dataset
- [x] 2.1.2.1 a conversation with andrew ng.en.srt
- [x] 2.1.2.12 week 1 wrap up.en.srt
- [ ] 2.1.2.3 training with the cats vs dogs dataset.en.srt
- [ ] 2.1.2.5 working through the notebook.en.srt
- [x] 2.1.2.7 fixing through cropping.en.srt
- [x] 2.1.2.8 visualizing the effect of the convolutions.en.srt
- [ ] 2.1.2.9 looking at accuracy and loss.en.srt

## 2.2 augmentation-a-technique-to-avoid-overfitting

## 2.2.1_augmentation
- [x] 2.2.1.1 a conversation with andrew ng.en.srt
- [ ] 2.2.1.11 exploring augmentation with horses vs humans.en.srt
- [x] 2.2.1.14 week 2 wrap up.en.srt
- [x] 2.2.1.3 introducing augmentation.en.srt
- [ ] 2.2.1.5 coding augmentation with imagedatagenerator.en.srt
- [ ] 2.2.1.7 demonstrating overfitting in cats vs dogs.en.srt
- [ ] 2.2.1.9 adding augmentation to cats vs dogs.en.srt

## 2.3 transfer-learning

## 2.3.1_transfer-learning
- [ ] 2.3.1.1 a conversation with andrew ng.en.srt
- [ ] 2.3.1.10 exploring transfer learning with inception.en.srt
- [ ] 2.3.1.13 week 3 wrap up.en.srt
- [x] 2.3.1.2 understanding transfer learning the concepts.en.srt
- [ ] 2.3.1.4 coding transfer learning from the inception mode.en.srt
- [ ] 2.3.1.6 coding your own model with transferred features.en.srt
- [ ] 2.3.1.8 exploring dropouts.en.srt

## 2.4 multiclass-classifications

## 2.4.1_multiclass-classifications
- [ ] 2.4.1.1 a conversation with andrew ng.en.srt
- [x] 2.4.1.2 moving from binary to multi class classification.en.srt
- [ ] 2.4.1.4 explore multi class with rock paper scissors dataset.en.srt
- [ ] 2.4.1.6 train a classifier with rock paper scissors.en.srt
- [ ] 2.4.1.8 test the rock paper scissors classifier.en.srt

## 2.4.2_course-2-wrap-up
- [ ] 2.4.2.2 a conversation with andrew ng.en.srt

# 3 natural-language-processing-tensorflow

## 3.1 sentiment-in-text

## 3.1.1_introduction
- [x] 3.1.1.1 introduction a conversation with andrew ng.en.srt

## 3.1.2_sentiment-in-text
- [ ] 3.1.2.1 introduction.en.srt
- [ ] 3.1.2.10 notebook for lesson 2.en.srt
- [ ] 3.1.2.11 sarcasm really.en.srt
- [ ] 3.1.2.12 working with the tokenizer.en.srt
- [ ] 3.1.2.15 notebook for lesson 3.en.srt
- [ ] 3.1.2.16 week 1 wrap up.en.srt
- [ ] 3.1.2.2 word based encodings.en.srt
- [ ] 3.1.2.3 using apis.en.srt
- [ ] 3.1.2.5 notebook for lesson 1.en.srt
- [ ] 3.1.2.6 text to sequence.en.srt
- [ ] 3.1.2.7 looking more at the tokenizer.en.srt
- [ ] 3.1.2.8 padding.en.srt

## 3.2 word-embeddings

## 3.2.1_word-embeddings
- [ ] 3.2.1.1 a conversation with andrew ng.en.srt
- [ ] 3.2.1.10 remember the sarcasm dataset.en.srt
- [ ] 3.2.1.11 building a classifier for the sarcasm dataset.en.srt
- [ ] 3.2.1.12 lets talk about the loss function.en.srt
- [ ] 3.2.1.14 pre tokenized datasets.en.srt
- [ ] 3.2.1.16 diving into the code part 1.en.srt
- [ ] 3.2.1.18 diving into the code part 2.en.srt
- [ ] 3.2.1.2 introduction.en.srt
- [ ] 3.2.1.20 notebook for lesson 3.en.srt
- [ ] 3.2.1.3 the imbd dataset.en.srt
- [ ] 3.2.1.5 looking into the details.en.srt
- [ ] 3.2.1.6 how can we use vectors.en.srt
- [ ] 3.2.1.7 more into the details.en.srt
- [ ] 3.2.1.9 notebook for lesson 1.en.srt

## 3.3 sequence-models

## 3.3.1_sequence-models
- [ ] 3.3.1.1 a conversation with andrew ng.en.srt
- [ ] 3.3.1.10 looking into the code.en.srt
- [ ] 3.3.1.11 using a convolutional network.en.srt
- [ ] 3.3.1.13 going back to the imdb dataset.en.srt
- [ ] 3.3.1.15 tips from laurence.en.srt
- [ ] 3.3.1.2 introduction.en.srt
- [ ] 3.3.1.4 lstms.en.srt
- [ ] 3.3.1.6 implementing lstms in code.en.srt
- [ ] 3.3.1.8 accuracy and loss.en.srt
- [ ] 3.3.1.9 a word from laurence.en.srt

## 3.4 sequence-models-and-literature

## 3.4.1_sequence-models-and-literature
- [ ] 3.4.1.1 a conversation with andrew ng.en.srt
- [ ] 3.4.1.10 predicting a word.en.srt
- [ ] 3.4.1.11 poetry.en.srt
- [ ] 3.4.1.13 looking into the code.en.srt
- [ ] 3.4.1.14 laurence the poet.en.srt
- [ ] 3.4.1.16 your next task.en.srt
- [ ] 3.4.1.2 introduction.en.srt
- [ ] 3.4.1.3 looking into the code.en.srt
- [ ] 3.4.1.4 training the data.en.srt
- [ ] 3.4.1.5 more on training the data.en.srt
- [ ] 3.4.1.7 notebook for lesson 1.en.srt
- [ ] 3.4.1.8 finding what the next word should be.en.srt
- [ ] 3.4.1.9 example.en.srt

## 3.4.2_course-3-wrap-up
- [ ] 3.4.2.2 a conversation with andrew ng.en.srt

# 4 tensorflow-sequences-time-series-and-prediction

## 4.1 sequences-and-prediction

## 4.1.1_introduction
- [ ] 4.1.1.1 introduction a conversation with andrew ng.en.srt

## 4.1.2_sequences-and-prediction
- [ ] 4.1.2.1 time series examples.en.srt
- [ ] 4.1.2.10 forecasting.en.srt
- [ ] 4.1.2.2 machine learning applied to time series.en.srt
- [ ] 4.1.2.3 common patterns in time series.en.srt
- [ ] 4.1.2.4 introduction to time series.en.srt
- [ ] 4.1.2.6 train validation and test sets.en.srt
- [ ] 4.1.2.7 metrics for evaluating performance.en.srt
- [ ] 4.1.2.8 moving average and differencing.en.srt
- [ ] 4.1.2.9 trailing versus centered windows.en.srt

## 4.2 deep-neural-networks-for-time-series

## 4.2.1_deep-neural-networks-for-time-series
- [ ] 4.2.1.1 a conversation with andrew ng.en.srt
- [ ] 4.2.1.10 more on single layer neural network.en.srt
- [ ] 4.2.1.12 deep neural network training tuning and prediction.en.srt
- [ ] 4.2.1.13 deep neural network.en.srt
- [ ] 4.2.1.2 preparing features and labels.en.srt
- [ ] 4.2.1.3 preparing features and labels.en.srt
- [ ] 4.2.1.6 feeding windowed dataset into neural network.en.srt
- [ ] 4.2.1.7 single layer neural network.en.srt
- [ ] 4.2.1.8 machine learning on time windows.en.srt
- [ ] 4.2.1.9 prediction.en.srt

## 4.3 recurrent-neural-networks-for-time-series

## 4.3.1_recurrent-neural-networks-for-time-series
- [ ] 4.3.1.10 lstm.en.srt
- [ ] 4.3.1.12 coding lstms.en.srt
- [ ] 4.3.1.13 more on lstm.en.srt
- [ ] 4.3.1.2 conceptual overview.en.srt
- [ ] 4.3.1.3 shape of the inputs to the rnn.en.srt
- [ ] 4.3.1.4 outputting a sequence.en.srt
- [ ] 4.3.1.5 lambda layers.en.srt
- [ ] 4.3.1.6 adjusting the learning rate dynamically.en.srt
- [ ] 4.3.1.8 rnn.en.srt

## 4.4 real-world-time-series-data

## 4.4.1_real-world-time-series-data
- [ ] 4.4.1.1 week 4 a conversation with andrew ng.en.srt
- [ ] 4.4.1.10 prediction.en.srt
- [ ] 4.4.1.11 sunspots.en.srt
- [ ] 4.4.1.13 combining our tools for analysis.en.srt
- [ ] 4.4.1.2 convolutions.en.srt
- [ ] 4.4.1.4 bi directional lstms.en.srt
- [ ] 4.4.1.6 lstm.en.srt
- [ ] 4.4.1.8 real data sunspots.en.srt
- [ ] 4.4.1.9 train and tune the model.en.srt

## 4.4.2_course-4-wrap-up
- [ ] 4.4.2.2 congratulations.en.srt

## 4.4.3_tensorflow-in-practice-has-come-to-an-end
- [ ] 4.4.3.1 specialization wrap up a conversation with andrew ng.en.srt
